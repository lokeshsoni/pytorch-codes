{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "import fc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('/home/jupyter/pytorch-codes/datasets/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('/home/jupyter/pytorch-codes/datasets/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 233,
       "width": 233
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = next(iter(trainloader))\n",
    "helper.imshow(image[0,:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fc_model.Network(784, 10, [512, 256, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2..  Training Loss: 2.304..  Test Loss: 2.299..  Test Accuracy: 0.130\n",
      "Epoch: 1/2..  Training Loss: 2.304..  Test Loss: 2.299..  Test Accuracy: 0.130\n",
      "Epoch: 2/2..  Training Loss: 2.304..  Test Loss: 2.299..  Test Accuracy: 0.130\n",
      "Epoch: 2/2..  Training Loss: 2.303..  Test Loss: 2.299..  Test Accuracy: 0.130\n"
     ]
    }
   ],
   "source": [
    "fc_model.train(model, trainloader, testloader, criterion, optimizer, epochs=2, print_every=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (hidden_layers): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  )\n",
       "  (output): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.5)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'fmnist_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('fmnist_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys() == model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`load_state_dict`** works only for model with same architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Network:\n\tsize mismatch for hidden_layers.0.weight: copying a param of torch.Size([400, 784]) from checkpoint, where the shape is torch.Size([512, 784]) in current model.\n\tsize mismatch for hidden_layers.0.bias: copying a param of torch.Size([400]) from checkpoint, where the shape is torch.Size([512]) in current model.\n\tsize mismatch for hidden_layers.1.weight: copying a param of torch.Size([200, 400]) from checkpoint, where the shape is torch.Size([256, 512]) in current model.\n\tsize mismatch for hidden_layers.1.bias: copying a param of torch.Size([200]) from checkpoint, where the shape is torch.Size([256]) in current model.\n\tsize mismatch for hidden_layers.2.weight: copying a param of torch.Size([100, 200]) from checkpoint, where the shape is torch.Size([128, 256]) in current model.\n\tsize mismatch for hidden_layers.2.bias: copying a param of torch.Size([100]) from checkpoint, where the shape is torch.Size([128]) in current model.\n\tsize mismatch for output.weight: copying a param of torch.Size([10, 100]) from checkpoint, where the shape is torch.Size([10, 128]) in current model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-5530b1a255ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 719\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Network:\n\tsize mismatch for hidden_layers.0.weight: copying a param of torch.Size([400, 784]) from checkpoint, where the shape is torch.Size([512, 784]) in current model.\n\tsize mismatch for hidden_layers.0.bias: copying a param of torch.Size([400]) from checkpoint, where the shape is torch.Size([512]) in current model.\n\tsize mismatch for hidden_layers.1.weight: copying a param of torch.Size([200, 400]) from checkpoint, where the shape is torch.Size([256, 512]) in current model.\n\tsize mismatch for hidden_layers.1.bias: copying a param of torch.Size([200]) from checkpoint, where the shape is torch.Size([256]) in current model.\n\tsize mismatch for hidden_layers.2.weight: copying a param of torch.Size([100, 200]) from checkpoint, where the shape is torch.Size([128, 256]) in current model.\n\tsize mismatch for hidden_layers.2.bias: copying a param of torch.Size([100]) from checkpoint, where the shape is torch.Size([128]) in current model.\n\tsize mismatch for output.weight: copying a param of torch.Size([10, 100]) from checkpoint, where the shape is torch.Size([10, 128]) in current model."
     ]
    }
   ],
   "source": [
    "model = fc_model.Network(784, 10, [400, 200, 100])\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save model state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {'input_size': 784,\n",
    "              'output_size': 10,\n",
    "              'hidden_layers': [each.out_features for each in model.hidden_layers],\n",
    "              'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, 'fmnist_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model = fc_model.Network(checkpoint['input_size'],\n",
    "                             checkpoint['output_size'],\n",
    "                             checkpoint['hidden_layers'])\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=400, bias=True)\n",
      "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
      "    (2): Linear(in_features=200, out_features=100, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = load_checkpoint('fmnist_checkpoint.pth')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
